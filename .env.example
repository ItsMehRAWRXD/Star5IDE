# Ollama Configuration
# Default Ollama host (change if your Ollama instance is running on a different address)
OLLAMA_HOST=http://localhost:11434

# Default model to use (examples: llama2, mistral, codellama, etc.)
OLLAMA_DEFAULT_MODEL=llama2

# Generation parameters
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2048